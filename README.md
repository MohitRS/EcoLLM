# Leveraging Knowledge Distillation and Advanced Compression Techniques for Sustainable LLM Training and Inference

This repository is dedicated to developing and sharing research on sustainable training methods for large language models (LLMs). Our focus is on reducing the environmental impact of AI technologies through innovative knowledge distillation and advanced compression techniques.

## Introduction
In the face of growing environmental concerns, there is a pressing need to reduce the carbon footprint associated with training large-scale AI models. This project investigates effective strategies to maintain high performance while minimizing energy consumption using methods like quantization, pruning, and Lora.

## Getting Started
Follow these instructions to set up the project environment and run the first example:

### Prerequisites
- Python 3.8 or higher
- PyTorch 1.7 or higher
- CUDA Toolkit 11.0 or higher (for GPU support)

### Installation
```bash
git clone https://github.com/MohitRS/EcoLLM
cd EcoLLM
pip install -r requirements.txt
